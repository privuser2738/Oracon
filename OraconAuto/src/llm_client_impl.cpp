#include "oracon/auto/llm_client.h"
#include "oracon/core/logger.h"
#include <sstream>
#include <thread>
#include <chrono>

namespace oracon {
namespace auto_ns {

// ===== Mock LLM Client for Testing =====

class MockLLMClient : public LLMClient {
public:
    MockLLMClient(const String& model = "mock-model")
        : m_model(model)
        , m_responseTemplate("Mock response to: {query}")
    {}

    LLMResponse complete(
        const std::vector<Message>& messages,
        const GenerationParams& params
    ) override {
        (void)params;  // Unused in mock

        LLMResponse response;
        response.model = m_model;
        response.success = true;

        // Build mock response based on last user message
        String lastUserMessage;
        for (const auto& msg : messages) {
            if (msg.role == Role::User) {
                lastUserMessage = msg.content;
            }
        }

        // Generate mock response
        response.content = generateMockResponse(lastUserMessage);

        // Mock token counts (rough estimate)
        response.promptTokens = estimateTokens(messages);
        response.completionTokens = static_cast<u32>(response.content.length() / 4);
        response.totalTokens = response.promptTokens + response.completionTokens;
        response.finishReason = "stop";

        return response;
    }

    LLMResponse streamComplete(
        const std::vector<Message>& messages,
        StreamCallback callback,
        const GenerationParams& params
    ) override {
        // Get complete response first
        LLMResponse response = complete(messages, params);

        // Simulate streaming by sending chunks
        const String& content = response.content;
        const size_t chunkSize = 10;

        for (size_t i = 0; i < content.length(); i += chunkSize) {
            String chunk = content.substr(i, chunkSize);
            callback(chunk);

            // Simulate network delay
            std::this_thread::sleep_for(std::chrono::milliseconds(50));
        }

        return response;
    }

    String getModelName() const override {
        return m_model;
    }

    bool isAvailable() const override {
        return true;  // Mock is always available
    }

    void setResponseTemplate(const String& template_str) {
        m_responseTemplate = template_str;
    }

private:
    String m_model;
    String m_responseTemplate;

    String generateMockResponse(const String& query) {
        std::ostringstream oss;
        oss << "Mock LLM Response:\n\n";
        oss << "You asked: \"" << query << "\"\n\n";
        oss << "This is a simulated response from " << m_model << ". ";
        oss << "In a real implementation, this would be generated by an actual LLM. ";
        oss << "The response would be contextually relevant to your query.";
        return oss.str();
    }

    u32 estimateTokens(const std::vector<Message>& messages) {
        u32 total = 0;
        for (const auto& msg : messages) {
            total += static_cast<u32>(msg.content.length() / 4);
        }
        return total;
    }
};

// Forward declare Anthropic client factory
std::unique_ptr<LLMClient> createAnthropicClient(
    const String& apiKey,
    const String& model,
    const String& baseUrl
);

// ===== Factory Implementation =====

std::unique_ptr<LLMClient> LLMClientFactory::create(
    Provider provider,
    const String& apiKey,
    const String& model,
    const String& baseUrl
) {
    switch (provider) {
        case Provider::Mock:
            return std::make_unique<MockLLMClient>(
                model.empty() ? "mock-gpt-4" : model
            );

        case Provider::OpenAI:
            ORACON_LOG_ERROR("OpenAI client not yet implemented");
            return std::make_unique<MockLLMClient>("mock-gpt-4");

        case Provider::Anthropic:
            return createAnthropicClient(apiKey, model, baseUrl);

        case Provider::Local:
            ORACON_LOG_ERROR("Local client not yet implemented");
            return std::make_unique<MockLLMClient>("mock-local");

        default:
            ORACON_LOG_ERROR("Unknown provider");
            return std::make_unique<MockLLMClient>("mock-unknown");
    }
}

} // namespace auto_ns
} // namespace oracon
